{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c28a6ec",
   "metadata": {},
   "source": [
    "# Convert the original audio to one containing only drums"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6e8f8c",
   "metadata": {},
   "source": [
    "## Method 1: training a model by ourselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f79bad0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import scipy.io.wavfile as wavwrite\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1768db66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a neural network model for drum separation\n",
    "class DrumSeparationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DrumSeparationModel, self).__init__()\n",
    "        # First convolutional layer\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        # Second convolutional layer\n",
    "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=2, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the network\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d4054f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset for drum separation\n",
    "class DrumDataset(Dataset):\n",
    "    def __init__(self, original_path, separated_path, file_list, max_length):\n",
    "        self.original_path = original_path\n",
    "        self.separated_path = separated_path\n",
    "        self.file_list = file_list\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load original and separated waveforms from audio files\n",
    "        original_file = os.path.join(self.original_path, self.file_list[idx])\n",
    "        separated_file = os.path.join(self.separated_path, self.file_list[idx])\n",
    "        original_waveform, _ = torchaudio.load(original_file)\n",
    "        separated_waveform, _ = torchaudio.load(separated_file)\n",
    "        \n",
    "        # Trim or pad waveforms to the specified maximum length\n",
    "        if original_waveform.size(1) > self.max_length:\n",
    "            original_waveform = original_waveform[:, :self.max_length]\n",
    "            separated_waveform = separated_waveform[:, :self.max_length]\n",
    "        else:\n",
    "            pad_length = self.max_length - original_waveform.size(1)\n",
    "            original_waveform = torch.nn.functional.pad(original_waveform, (0, pad_length))\n",
    "            separated_waveform = torch.nn.functional.pad(separated_waveform, (0, pad_length))\n",
    "\n",
    "        # Ensure there is only one channel\n",
    "        original_waveform = original_waveform[0:1, :]\n",
    "        return original_waveform, separated_waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4b0cb314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Root Mean Squared Error (RMSE)\n",
    "def calculate_rmse(predictions, targets):\n",
    "    rmse = torch.sqrt(torch.mean((predictions - targets)**2))\n",
    "    return rmse.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c22c052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths for training and validation datasets\n",
    "train_original_path = \"C:\\\\Users\\\\chaoy\\\\Downloads\\\\DSD100\\\\DSD100\\\\Mixtures\\\\Dev\"\n",
    "train_separated_path = \"C:\\\\Users\\\\chaoy\\\\Downloads\\\\DSD100\\\\DSD100\\\\Sources\\\\Dev\"\n",
    "val_original_path = \"C:\\\\Users\\\\chaoy\\\\Downloads\\\\DSD100\\\\DSD100\\\\Mixtures\\\\Test\"\n",
    "val_separated_path = \"C:\\\\Users\\\\chaoy\\\\Downloads\\\\DSD100\\\\DSD100\\\\Sources\\\\Test\"\n",
    "\n",
    "# List all audio file names\n",
    "train_files = list(Path(train_original_path).rglob('**/mixture.wav'))\n",
    "val_files = list(Path(val_original_path).rglob('**/mixture.wav'))\n",
    "\n",
    "# Assume file names have a one-to-one correspondence\n",
    "assert len(train_files) == len(val_files)\n",
    "\n",
    "# Split file names into training and validation sets\n",
    "train_files, val_files = train_files, val_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "092c874a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instances of training and validation datasets\n",
    "max_length = 1133393   # Set a maximum length\n",
    "train_dataset = DrumDataset(train_original_path, train_separated_path, train_files, max_length)\n",
    "val_dataset = DrumDataset(val_original_path, val_separated_path, val_files, max_length)\n",
    "\n",
    "# Use DataLoader to load datasets\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize the drum separation model\n",
    "model = DrumSeparationModel()\n",
    "\n",
    "# Initialize loss function and optimizer\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "59a3dc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    for original_waveform, separated_waveform in train_loader:\n",
    "        # Forward pass\n",
    "        predictions = model(original_waveform)\n",
    "        # Compute loss\n",
    "        loss = criterion(predictions, separated_waveform)\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "321ea374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE: 0.005262430310249328\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    total_rmse = 0.0\n",
    "    total_samples = 0\n",
    "    for original_waveform, separated_waveform in val_loader:\n",
    "        # Forward pass\n",
    "        predictions = model(original_waveform)\n",
    "         # Calculate RMSE\n",
    "        rmse = calculate_rmse(predictions, separated_waveform)\n",
    "        total_rmse += rmse\n",
    "        total_samples += original_waveform.size(0)\n",
    "    accuracy = total_rmse / total_samples\n",
    "    print(f'Validation RMSE: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cac9df7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a new audio file for drum separation\n",
    "new_audio_file = \"drums3.wav\"\n",
    "new_waveform, _ = torchaudio.load(new_audio_file)\n",
    "\n",
    "# Trim or pad the waveform to the specified maximum length\n",
    "if new_waveform.size(1) > max_length:\n",
    "    new_waveform = new_waveform[:, :max_length]\n",
    "else:\n",
    "    pad_length = max_length - new_waveform.size(1)\n",
    "    new_waveform = torch.nn.functional.pad(new_waveform, (0, pad_length))\n",
    "\n",
    "# Ensure there is only one channel\n",
    "new_waveform = new_waveform[0:1, :]\n",
    "new_waveform = new_waveform.unsqueeze(0) \n",
    "\n",
    "# Make predictions using the trained model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predicted_waveform = model(new_waveform)\n",
    "    predicted_waveform = predicted_waveform.squeeze(0)\n",
    "\n",
    "# Scale the floating-point tensor to the range of 16-bit integers\n",
    "predicted_waveform_int = (predicted_waveform * 32767).to(torch.int16)\n",
    "\n",
    "# Save the predicted audio as a WAV file\n",
    "predicted_audio_file_wav = \"drums4.wav\"\n",
    "wavwrite.write(predicted_audio_file_wav, 44100, predicted_waveform_int.numpy().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f814661",
   "metadata": {},
   "source": [
    "## Method 2: separate audio tracks with Demucs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8043f81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from demucs import pretrained\n",
    "from demucs.apply import apply_model\n",
    "import torchaudio\n",
    "import os\n",
    "\n",
    "def separate_sources(input_file):\n",
    "    # Load the pretrained Demucs model\n",
    "    model = pretrained.get_model('mdx')\n",
    "\n",
    "    # Load the audio file\n",
    "    waveform, sample_rate = torchaudio.load(input_file)\n",
    "\n",
    "    # Separate sources in the audio\n",
    "    sources = apply_model(model, waveform.unsqueeze(0))  # Add a dimension before mix\n",
    "\n",
    "    # Save only the first channel of the first source\n",
    "    output_file = \"drums2.wav\"\n",
    "    torchaudio.save(output_file, sources[0][0], sample_rate)\n",
    "\n",
    "# Set input file and output file prefix\n",
    "input_file = \"input_music.wav\"  \n",
    "\n",
    "# Execute source separation\n",
    "separate_sources(input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7328f626",
   "metadata": {},
   "source": [
    "# Converting the .wav file to .txt file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619cb8aa",
   "metadata": {},
   "source": [
    "## Part 1: an original audio file to audio information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "43258932",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average audio information with max and min values written to audio.txt\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "\n",
    "# Load the wav file\n",
    "wav_file = 'input_music.wav'\n",
    "y, sr = librosa.load(wav_file)\n",
    "\n",
    "# Set the desired hop length and bins per octave\n",
    "hop_length = 512\n",
    "bins_per_octave = 12\n",
    "\n",
    "# Compute the constant-Q chromagram\n",
    "CQT = librosa.amplitude_to_db(np.abs(librosa.cqt(y, sr=sr, hop_length=hop_length, bins_per_octave=bins_per_octave)), ref=np.max)\n",
    "\n",
    "# Get the time and frequency bins\n",
    "times = librosa.times_like(CQT)\n",
    "frequencies = librosa.cqt_frequencies(n_bins=CQT.shape[0], fmin=librosa.note_to_hz('C1'), bins_per_octave=bins_per_octave)\n",
    "\n",
    "# Convert the amplitude spectrogram to strength\n",
    "strength = librosa.power_to_db(np.abs(librosa.stft(y, hop_length=hop_length)), ref=np.max)\n",
    "\n",
    "# Calculate the average strength and frequency for each time frame\n",
    "average_strength = np.mean(strength, axis=0)\n",
    "average_frequency = np.sum(np.exp(CQT) * frequencies[:, None], axis=0) / np.sum(np.exp(CQT), axis=0)\n",
    "\n",
    "# Calculate max and min values for average strength and frequency\n",
    "max_avg_strength = np.max(average_strength)\n",
    "min_avg_strength = np.min(average_strength)\n",
    "max_avg_frequency = np.max(average_frequency)\n",
    "min_avg_frequency = np.min(average_frequency)\n",
    "\n",
    "# Write max and min values to a text file\n",
    "output_file = 'audio.txt'\n",
    "with open(output_file, 'w') as file:\n",
    "    file.write(f\"{max_avg_frequency:.2f}\\n\")\n",
    "    file.write(f\"{min_avg_frequency:.2f}\\n\")\n",
    "    file.write(f\"{max_avg_strength:.2f}\\n\")\n",
    "    file.write(f\"{min_avg_strength:.2f}\\n\")\n",
    "    for i, time in enumerate(times):\n",
    "        file.write(f\"{time:.1f} {average_frequency[i]:.2f} {average_strength[i]:.2f}\\n\")\n",
    "\n",
    "print(f\"Average audio information with max and min values written to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cfc836",
   "metadata": {},
   "source": [
    "## Part 2: a drums predicted file to drums information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "04454223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rounded drum onset times (in milliseconds, rounded to the nearest 100ms) written to drums.txt\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "\n",
    "# Read the output drum source file\n",
    "drum_file = \"drums.wav\" \n",
    "\n",
    "# Load the audio file\n",
    "drum_data, drum_sr = librosa.load(drum_file)\n",
    "\n",
    "# Extract drum onsets\n",
    "onset_frames = librosa.onset.onset_detect(y=drum_data, sr=drum_sr)\n",
    "onset_times_ms = librosa.frames_to_time(onset_frames, sr=drum_sr, hop_length=512) * 1000  # Convert time to milliseconds\n",
    "\n",
    "# Write drum onset times to a text file (rounded to the nearest 100 milliseconds)\n",
    "output_txt_file = \"drums.txt\"  \n",
    "\n",
    "with open(output_txt_file, 'w') as f:\n",
    "    for onset_time_ms in onset_times_ms:\n",
    "        rounded_onset_time_ms = round(onset_time_ms / 100) * 100  # Round to the nearest 100 milliseconds\n",
    "        f.write(f\"{rounded_onset_time_ms}\\n\")\n",
    "\n",
    "print(f\"Rounded drum onset times (in milliseconds, rounded to the nearest 100ms) written to {output_txt_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
